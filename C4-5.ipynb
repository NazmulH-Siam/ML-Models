{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbFEftLL5G2xOk3BZO9hzB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rZE4vpdmmId","executionInfo":{"status":"ok","timestamp":1722217758376,"user_tz":180,"elapsed":3606,"user":{"displayName":"Nazmul H. Siam","userId":"01851349013009671388"}},"outputId":"2e948d57-79f7-4391-9fc1-fd167bfc6ab4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["pip install pandas numpy scikit-learn nltk scipy"]},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.tree import DecisionTreeClassifier\n","from scipy.sparse import hstack, csr_matrix\n","import gc\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Download NLTK data\n","nltk.download('punkt')\n","\n","# Load the dataset\n","file_path = '/content/PhiUSIIL_Phishing_URL_Dataset (updated 3-3-24 ).csv'\n","df = pd.read_csv(file_path)\n","\n","# Handle missing values\n","# Fill missing numerical values with the median value of each column\n","numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n","numerical_cols = numerical_cols.drop('label')  # Exclude the label column\n","df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n","\n","# Fill missing categorical values with the mode value of each column\n","categorical_cols = df.select_dtypes(include=['object']).columns\n","df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n","\n","# Check for and handle missing values in the target column\n","if df['label'].isnull().any():\n","    print(\"Found NaN values in the target column. Filling with mode value.\")\n","    df['label'] = df['label'].fillna(df['label'].mode()[0])\n","\n","# Encode categorical variables\n","# Example: Encoding the 'TLD' column\n","le = LabelEncoder()\n","df['TLD'] = le.fit_transform(df['TLD'])\n","\n","# Tokenization and Hashing Vectorizer for text columns\n","text_cols = ['FILENAME', 'URL', 'Domain', 'Title']\n","hashing_vectorizer = HashingVectorizer(n_features=2**10, alternate_sign=False)  # Reduced number of features for efficiency\n","\n","# Apply tokenization and vectorization\n","text_features = [hashing_vectorizer.transform(df[col].apply(lambda x: ' '.join(word_tokenize(x)))) for col in text_cols]\n","\n","# Combine text features into a single sparse matrix\n","text_features_combined = hstack(text_features)\n","\n","# Separate numerical features and target\n","X_numerical = df[numerical_cols].values\n","y = df['label'].values\n","\n","# Standardize numerical features\n","scaler = StandardScaler()\n","X_numerical = scaler.fit_transform(X_numerical)\n","\n","# Combine numerical and text features\n","X = hstack([csr_matrix(X_numerical), text_features_combined])\n","\n","# Reduce dimensionality with Truncated SVD\n","svd = TruncatedSVD(n_components=100, random_state=42)\n","X_reduced = svd.fit_transform(X)\n","\n","# Adjust labels for one-hot encoding\n","y_adjusted = y - 1\n","\n","# 10-fold cross-validation\n","skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","accuracy_scores = []\n","precision_scores = []\n","recall_scores = []\n","f1_scores = []\n","classification_reports = []\n","\n","for fold, (train_index, test_index) in enumerate(skf.split(X_reduced, y)):\n","    X_train, X_test = X_reduced[train_index], X_reduced[test_index]\n","    y_train, y_test = y_adjusted[train_index], y_adjusted[test_index]\n","\n","    print(f\"Fold {fold + 1}:\")\n","    print(f\"Training data size: {len(train_index)}\")\n","    print(f\"Testing data size: {len(test_index)}\")\n","\n","    # Initialize the Decision Tree model\n","    model = DecisionTreeClassifier(criterion='entropy')\n","\n","    # Train the model\n","    model.fit(X_train, y_train)\n","\n","    # Predict using the trained model\n","    y_pred_test = model.predict(X_test)\n","\n","    # Adjust predicted labels back to original range (1 and 2)\n","    y_pred_test_labels_adjusted = y_pred_test + 1\n","    y_test_labels_adjusted = y_test + 1\n","\n","    # Evaluate the model\n","    accuracy = accuracy_score(y_test_labels_adjusted, y_pred_test_labels_adjusted)\n","    precision = precision_score(y_test_labels_adjusted, y_pred_test_labels_adjusted, average='weighted')\n","    recall = recall_score(y_test_labels_adjusted, y_pred_test_labels_adjusted, average='weighted')\n","    f1 = f1_score(y_test_labels_adjusted, y_pred_test_labels_adjusted, average='weighted')\n","    report = classification_report(y_test_labels_adjusted, y_pred_test_labels_adjusted, output_dict=True)\n","\n","    accuracy_scores.append(accuracy)\n","    precision_scores.append(precision)\n","    recall_scores.append(recall)\n","    f1_scores.append(f1)\n","    classification_reports.append(report)\n","\n","    print(f\"Fold {fold + 1} Results:\")\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Precision: {precision}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"F1 Score: {f1}\\n\")\n","\n","    # Free up memory\n","    del X_train, X_test, y_train, y_test, model, y_pred_test\n","    gc.collect()\n","\n","# Aggregate the classification reports\n","average_classification_report = {}\n","for key in classification_reports[0].keys():\n","    if isinstance(classification_reports[0][key], dict):\n","        average_classification_report[key] = {}\n","        for sub_key in classification_reports[0][key].keys():\n","            average_classification_report[key][sub_key] = np.mean([report[key][sub_key] for report in classification_reports])\n","    else:\n","        average_classification_report[key] = np.mean([report[key] for report in classification_reports])\n","\n","# Calculate average metrics\n","average_accuracy = np.mean(accuracy_scores)\n","average_precision = np.mean(precision_scores)\n","average_recall = np.mean(recall_scores)\n","average_f1_score = np.mean(f1_scores)\n","\n","print(f\"Average Model Accuracy: {average_accuracy}\")\n","print(f\"Average Precision: {average_precision}\")\n","print(f\"Average Recall: {average_recall}\")\n","print(f\"Average F1 Score: {average_f1_score}\")\n","print(\"Average Classification Report:\")\n","for key, value in average_classification_report.items():\n","    if isinstance(value, dict):\n","        print(f\"  {key}:\")\n","        for sub_key, sub_value in value.items():\n","            print(f\"    {sub_key}: {sub_value}\")\n","    else:\n","        print(f\"  {key}: {value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26djPqiWrHug","executionInfo":{"status":"ok","timestamp":1722218675562,"user_tz":180,"elapsed":617533,"user":{"displayName":"Nazmul H. Siam","userId":"01851349013009671388"}},"outputId":"bd8df8a0-78da-425a-b509-a2824dea2eb6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Fold 1:\n","Training data size: 212215\n","Testing data size: 23580\n","Fold 1 Results:\n","Accuracy: 0.9991942324003392\n","Precision: 0.9991944822700535\n","Recall: 0.9991942324003392\n","F1 Score: 0.9991942674395862\n","\n","Fold 2:\n","Training data size: 212215\n","Testing data size: 23580\n","Fold 2 Results:\n","Accuracy: 0.9986429177268872\n","Precision: 0.9986429177268872\n","Recall: 0.9986429177268872\n","F1 Score: 0.9986429177268872\n","\n","Fold 3:\n","Training data size: 212215\n","Testing data size: 23580\n","Fold 3 Results:\n","Accuracy: 0.9986005089058524\n","Precision: 0.9986005664080094\n","Recall: 0.9986005089058524\n","F1 Score: 0.9986004477857106\n","\n","Fold 4:\n","Training data size: 212215\n","Testing data size: 23580\n","Fold 4 Results:\n","Accuracy: 0.998854961832061\n","Precision: 0.9988550375784992\n","Recall: 0.998854961832061\n","F1 Score: 0.9988549831980469\n","\n","Fold 5:\n","Training data size: 212215\n","Testing data size: 23580\n","Fold 5 Results:\n","Accuracy: 0.9989397794741306\n","Precision: 0.9989398520581593\n","Recall: 0.9989397794741306\n","F1 Score: 0.9989397992574507\n","\n","Fold 6:\n","Training data size: 212216\n","Testing data size: 23579\n","Fold 6 Results:\n","Accuracy: 0.9990669663683787\n","Precision: 0.9990669785446354\n","Recall: 0.9990669663683787\n","F1 Score: 0.9990669430958016\n","\n","Fold 7:\n","Training data size: 212216\n","Testing data size: 23579\n","Fold 7 Results:\n","Accuracy: 0.9991941982272361\n","Precision: 0.9991942119314505\n","Recall: 0.9991941982272361\n","F1 Score: 0.9991942032442843\n","\n","Fold 8:\n","Training data size: 212216\n","Testing data size: 23579\n","Fold 8 Results:\n","Accuracy: 0.9990669663683787\n","Precision: 0.9990670286375449\n","Recall: 0.9990669663683787\n","F1 Score: 0.9990669314380767\n","\n","Fold 9:\n","Training data size: 212216\n","Testing data size: 23579\n","Fold 9 Results:\n","Accuracy: 0.9987276814114254\n","Precision: 0.9987276814114254\n","Recall: 0.9987276814114254\n","F1 Score: 0.9987276814114254\n","\n","Fold 10:\n","Training data size: 212216\n","Testing data size: 23579\n","Fold 10 Results:\n","Accuracy: 0.9991093769879978\n","Precision: 0.999109443264992\n","Recall: 0.9991093769879978\n","F1 Score: 0.9991093936132447\n","\n","Average Model Accuracy: 0.9989397589702687\n","Average Precision: 0.9989398199831656\n","Average Recall: 0.9989397589702687\n","Average F1 Score: 0.9989397568210514\n","Average Classification Report:\n","  1:\n","    precision: 0.9987617581274622\n","    recall: 0.9987617043672588\n","    f1-score: 0.998761687626214\n","    support: 10094.5\n","  2:\n","    precision: 0.999073114969471\n","    recall: 0.9990730441230996\n","    f1-score: 0.9990730550991996\n","    support: 13485.0\n","  accuracy: 0.9989397589702687\n","  macro avg:\n","    precision: 0.9989174365484667\n","    recall: 0.9989173742451793\n","    f1-score: 0.998917371362707\n","    support: 23579.5\n","  weighted avg:\n","    precision: 0.9989398199831656\n","    recall: 0.9989397589702687\n","    f1-score: 0.9989397568210514\n","    support: 23579.5\n"]}]}]}