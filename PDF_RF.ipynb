{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHr/3x0ADz9vlW+1jVP/Au"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kgw9oEjf7qJa","executionInfo":{"status":"ok","timestamp":1722214188612,"user_tz":180,"elapsed":16137,"user":{"displayName":"Nazmul H. Siam","userId":"01851349013009671388"}},"outputId":"229443e5-9bd8-40a5-de5e-872cd58c241d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["pip install pandas numpy scikit-learn nltk scipy"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestClassifier\n","from scipy.sparse import hstack, csr_matrix\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Download NLTK data\n","nltk.download('punkt')\n","\n","# Load the dataset\n","file_path = '/content/PDFMalware2022.csv'\n","df = pd.read_csv(file_path)\n","\n","# Handle missing values\n","# Fill missing numerical values with the median value of each column\n","numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n","numerical_cols = numerical_cols.drop('Class')  # Exclude the target column\n","df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n","\n","# Fill missing categorical values with the mode value of each column\n","categorical_cols = df.select_dtypes(include=['object']).columns\n","df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n","\n","# Check for and handle missing values in the target column\n","if df['Class'].isnull().any():\n","    print(\"Found NaN values in the target column. Filling with mode value.\")\n","    df['Class'] = df['Class'].fillna(df['Class'].mode()[0])\n","\n","# Tokenization and Hashing Vectorizer for text columns\n","text_cols = ['File name', 'text', 'header']\n","hashing_vectorizer = HashingVectorizer(n_features=2**14, alternate_sign=False)  # Reduced number of features for efficiency\n","\n","# Apply tokenization and vectorization\n","text_features = [hashing_vectorizer.transform(df[col].apply(lambda x: ' '.join(word_tokenize(x)))) for col in text_cols]\n","\n","# Combine text features into a single sparse matrix\n","text_features_combined = hstack(text_features)\n","\n","# Separate numerical features and target\n","X_numerical = df[numerical_cols].values\n","y = df['Class'].values\n","\n","# Standardize numerical features\n","scaler = StandardScaler()\n","X_numerical = scaler.fit_transform(X_numerical)\n","\n","# Combine numerical and text features\n","X = hstack([csr_matrix(X_numerical), text_features_combined])\n","\n","# Reduce dimensionality with Truncated SVD\n","svd = TruncatedSVD(n_components=30, random_state=42)  # Further reduced number of components for efficiency\n","X_reduced = svd.fit_transform(X)\n","\n","# Adjust labels for one-hot encoding\n","y_adjusted = y - 1\n","\n","# One-hot encode the target labels\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","y_one_hot = one_hot_encoder.fit_transform(y_adjusted.reshape(-1, 1))\n","\n","# Flatten the one-hot encoded labels for Random Forest\n","y_flat = np.argmax(y_one_hot, axis=1)\n","\n","# 10-fold cross-validation\n","skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","accuracy_scores = []\n","precision_scores = []\n","recall_scores = []\n","f1_scores = []\n","classification_reports = []\n","\n","for fold, (train_index, test_index) in enumerate(skf.split(X_reduced, y)):\n","    X_train, X_test = X_reduced[train_index], X_reduced[test_index]\n","    y_train, y_test = y_flat[train_index], y_flat[test_index]\n","\n","    # Define the Random Forest model with fewer estimators and enable parallel processing\n","    model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)  # Reduced number of trees\n","\n","    # Train the Random Forest model\n","    model.fit(X_train, y_train)\n","\n","    # Predict using the trained model\n","    y_pred_test = model.predict(X_test)\n","\n","    # Adjust predicted labels back to original range (1 and 2)\n","    y_pred_test_labels_adjusted = y_pred_test + 1\n","    y_test_labels_adjusted = y_test + 1\n","\n","    # Evaluate the model\n","    accuracy = accuracy_score(y_test_labels_adjusted, y_pred_test_labels_adjusted)\n","    precision = precision_score(y_test_labels_adjusted, y_pred_test_labels_adjusted, average='weighted')\n","    recall = recall_score(y_test_labels_adjusted, y_pred_test_labels_adjusted, average='weighted')\n","    f1 = f1_score(y_test_labels_adjusted, y_pred_test_labels_adjusted, average='weighted')\n","    report = classification_report(y_test_labels_adjusted, y_pred_test_labels_adjusted, output_dict=True)\n","\n","    accuracy_scores.append(accuracy)\n","    precision_scores.append(precision)\n","    recall_scores.append(recall)\n","    f1_scores.append(f1)\n","    classification_reports.append(report)\n","\n","    print(f\"Fold {fold + 1} Results:\")\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Precision: {precision}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"F1 Score: {f1}\\n\")\n","\n","# Aggregate the classification reports\n","average_classification_report = {}\n","for key in classification_reports[0].keys():\n","    if isinstance(classification_reports[0][key], dict):\n","        average_classification_report[key] = {}\n","        for sub_key in classification_reports[0][key].keys():\n","            average_classification_report[key][sub_key] = np.mean([report[key][sub_key] for report in classification_reports])\n","    else:\n","        average_classification_report[key] = np.mean([report[key] for report in classification_reports])\n","\n","# Calculate average metrics\n","average_accuracy = np.mean(accuracy_scores)\n","average_precision = np.mean(precision_scores)\n","average_recall = np.mean(recall_scores)\n","average_f1_score = np.mean(f1_scores)\n","\n","print(f\"Average Model Accuracy: {average_accuracy}\")\n","print(f\"Average Precision: {average_precision}\")\n","print(f\"Average Recall: {average_recall}\")\n","print(f\"Average F1 Score: {average_f1_score}\")\n","print(\"Average Classification Report:\")\n","for key, value in average_classification_report.items():\n","    if isinstance(value, dict):\n","        print(f\"  {key}:\")\n","        for sub_key, sub_value in value.items():\n","            print(f\"    {sub_key}: {sub_value}\")\n","    else:\n","        print(f\"  {key}: {value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cA1m7jvi7veD","executionInfo":{"status":"ok","timestamp":1722214259865,"user_tz":180,"elapsed":67040,"user":{"displayName":"Nazmul H. Siam","userId":"01851349013009671388"}},"outputId":"8f7c5649-4903-425b-e803-4c94cde70ed4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Found NaN values in the target column. Filling with mode value.\n","Fold 1 Results:\n","Accuracy: 0.9750747756729811\n","Precision: 0.9751808702876702\n","Recall: 0.9750747756729811\n","F1 Score: 0.9750480938985008\n","\n","Fold 2 Results:\n","Accuracy: 0.9780658025922233\n","Precision: 0.9782863648006926\n","Recall: 0.9780658025922233\n","F1 Score: 0.9780336768279901\n","\n","Fold 3 Results:\n","Accuracy: 0.9690927218344965\n","Precision: 0.9691060508314628\n","Recall: 0.9690927218344965\n","F1 Score: 0.9690749807288369\n","\n","Fold 4 Results:\n","Accuracy: 0.9680957128614157\n","Precision: 0.9680892484240324\n","Recall: 0.9680957128614157\n","F1 Score: 0.9680885851193631\n","\n","Fold 5 Results:\n","Accuracy: 0.9830508474576272\n","Precision: 0.9831202943728157\n","Recall: 0.9830508474576272\n","F1 Score: 0.983036981485871\n","\n","Fold 6 Results:\n","Accuracy: 0.9810568295114656\n","Precision: 0.9810617945515577\n","Recall: 0.9810568295114656\n","F1 Score: 0.9810504227379163\n","\n","Fold 7 Results:\n","Accuracy: 0.9730538922155688\n","Precision: 0.9730717424573411\n","Recall: 0.9730538922155688\n","F1 Score: 0.9730382540036405\n","\n","Fold 8 Results:\n","Accuracy: 0.9660678642714571\n","Precision: 0.9660904145808434\n","Recall: 0.9660678642714571\n","F1 Score: 0.9660752501889021\n","\n","Fold 9 Results:\n","Accuracy: 0.9840319361277445\n","Precision: 0.9841293634323297\n","Recall: 0.9840319361277445\n","F1 Score: 0.9840169763647915\n","\n","Fold 10 Results:\n","Accuracy: 0.9740518962075848\n","Precision: 0.9740598919429563\n","Recall: 0.9740518962075848\n","F1 Score: 0.9740401713266024\n","\n","Average Model Accuracy: 0.9751642278752563\n","Average Precision: 0.9752196035681701\n","Average Recall: 0.9751642278752563\n","Average F1 Score: 0.9751503392682416\n","Average Classification Report:\n","  1:\n","    precision: 0.9730888647777436\n","    recall: 0.9823692397433404\n","    f1-score: 0.9776961703899903\n","    support: 555.8\n","  2:\n","    precision: 0.9778675423515205\n","    recall: 0.9662031881702632\n","    f1-score: 0.9719829761181418\n","    support: 446.8\n","  accuracy: 0.9751642278752563\n","  macro avg:\n","    precision: 0.975478203564632\n","    recall: 0.9742862139568018\n","    f1-score: 0.974839573254066\n","    support: 1002.6\n","  weighted avg:\n","    precision: 0.9752196035681701\n","    recall: 0.9751642278752563\n","    f1-score: 0.9751503392682416\n","    support: 1002.6\n"]}]}]}